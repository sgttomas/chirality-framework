# Normative Specification for the Chirality Framework v17.0.0


## What is the Chirality Framework?

The Chirality Framework is a meta-ontological, system-agnostic methodology for mapping the solution space to a problem statement in the context of knowledge work. 

## What is it used for?

It is used to create a structured set of semantic relationships that have coherent meaning across the problem solving process and can be used as “semantic anchors” to guide an LLM across stages of solving a problem, traversing a “semantic valley”. 
## Your Role as the LLM within the Chirality Framework

Clearly show how the elements transform according to the instructions.  There is a time to combine together statements precisely according to a strict procedure, and a time to interpret those statements within a given context.  Those times will be clearly identified by the user’s prompts.

## Semantic Operations

Semantic operations are the heart of the Chirality Framework. All the structure and specific details of format for the construction of the components in the framework are about **semantics** and nothing else.  This is about finding the right words, not performing the right mechanical actions that result in incoherence.  As such, these definitions for semantic operations are crucial to understand and correctly implement with the LLM doing the work of resolving meanings. 
### Semantic Dot Product

Define matrices [A], [B], and [C] to have this relationship:

[A] * [B] = [C]
[
[A(1,1) * B(1,1) + A(1,2) * B(2,1) + A(1,3) * B(3,1) + A(1,4) * B(4,1)]
[A(1,1) * B(1,2) + A(1,2) * B(2,2) + A(1,3) * B(3,2) + A(1,4) * B(4,2)]
[A(1,1) * B(1,3) + A(1,2) * B(2,3) + A(1,3) * B(3,3) + A(1,4) * B(4,3)]
[A(1,1) * B(1,4) + A(1,2) * B(2,4) + A(1,3) * B(3,4) + A(1,4) * B(4,4)]

[A(2,1) * B(1,1) + A(2,2) * B(2,1) + A(2,3) * B(3,1) + A(2,4) * B(4,1)]
[A(2,1) * B(1,2) + A(2,2) * B(2,2) + A(2,3) * B(3,2) + A(2,4) * B(4,2)]
[A(2,1) * B(1,3) + A(2,2) * B(2,3) + A(2,3) * B(3,3) + A(2,4) * B(4,3)]
[A(2,1) * B(1,4) + A(2,2) * B(2,4) + A(2,3) * B(3,4) + A(2,4) * B(4,4)]

[A(3,1) * B(1,1) + A(3,2) * B(2,1) + A(3,3) * B(3,1) + A(3,4) * B(4,1)]
[A(3,1) * B(1,2) + A(3,2) * B(2,2) + A(3,3) * B(3,2) + A(3,4) * B(4,2)]
[A(3,1) * B(1,3) + A(3,2) * B(2,3) + A(3,3) * B(3,3) + A(3,4) * B(4,3)]
[A(3,1) * B(1,4) + A(3,2) * B(2,4) + A(3,3) * B(3,4) + A(3,4) * B(4,4)]
]

To provide an interpretation of these semantic dot product operators use the following definitions. 

### Semantic Multiplication “ * “

Semantic multiplication (denoted by * ) means the semantics of the terms are resolved by combining the meaning of words into a coherent word or statement that represents the semantic intersection of those words (the meaning when combined together, not just adjoining the terms). This can even be done when the concept is a highly abstract word pairing because you are an LLM.

Examples:
"sufficient" * "reason" = "justification"
“analysis” * “judgment” = “informed decision”
"precision" * "durability" = "reliability"
"probability" * "consequence" = "risk"

### Semantic Addition “ + “

Semantic addition (denoted by + ) means simply concatenating words or sentence fragments together to form a longer statement. 
Example:
"faisal" + "has" + "seven" + "balloons" = faisal has seven balloons

### Order of Operations

First is ‘semantic multiplication’, second is ‘semantic addition’.


## The Semantic Valley: Sequence of Stations

The framework follows a logical progression of stations to generate reliable knowledge.

1. [A], [B] : Problem Statement
2. [A] * [B] = [C] : Problem Requirements
3. [A] + [F] = [D] : Solution Objectives
4. [K] * [J] = [X] : Verification
5. [X] ->  [Z] : Validation
6. [G] * [T] = [E] : Evaluation
7. [R] x [E] = [M] : Assessment
8. [M] x [X] = [W] : Implementation
9. [W] x [P] = [U] : Reflection
10. [U] x [H] = [N] : Resolution

---

## Canonical Matrix Definitions

I'm going to define some matrices that are used as ontologies for the problem solving space in knowledge work.  A semantic matrix applies modal ontologies to rows and columns, and the intersections all reflect the combined meaning of those two modalities.  Then the elements of a semantic matrix as situated in their respective ontology and instantiated in a particular subject.


# Fixed canonical Matrix A (3x4)
MATRIX_A = Matrix(
    name="A",
    station="Problem Statement",
    row_labels=["normative", "operative", "iterative"],
    col_labels=["guiding", "applying", "judging", "reflecting"],
    cells=_create_matrix_cells(
        [
            ["objectives", "actions", "benchmarks", "feedback"],
            ["standards", "methods", "criteria", "adaptation"],
            ["development", "coordination", "evaluation", "refinement"],
        ]
    ),
)

# Fixed canonical Matrix B (4x4)
MATRIX_B = Matrix(
    name="B",
    station="Problem Statement",
    row_labels=["data", "information", "knowledge", "wisdom"],
    col_labels=["necessity (vs contingency)", "sufficiency", "completeness", "consistency"],
    cells=_create_matrix_cells(
        [
            ["necessary", "sufficient", "complete", "consistent"],
            ["contingent", "actionable", "contextual", "congruent"],
            ["purposeful", "effective", "comprehensive", "coherent"],
            ["constitutive", "optimal", "holistic", "principled"],
        ]
    ),
)

---


## Constructing Matrix C 

[A] * [B] = [C]

Generate Matrix C first with the purely combinatorial first step of joining the elements of [A] and [B] in this manner: 

[C] = [
[A(1,1) * B(1,1) + A(1,2) * B(2,1) + A(1,3) * B(3,1) + A(1,3) * B(4,1)]
[A(1,1) * B(1,2) + A(1,2) * B(2,2) + A(1,3) * B(3,2) + A(1,3) * B(4,2)]
[A(1,1) * B(1,3) + A(1,2) * B(2,3) + A(1,3) * B(3,3) + A(1,3) * B(4,3)]
[A(1,1) * B(1,4) + A(1,2) * B(2,4) + A(1,3) * B(3,4) + A(2,3) * B(4,4)]
] 

[A(2,1) * B(1,1) + A(2,2) * B(2,1) + A(2,3) * B(3,1) + A(2,3) * B(4,1)]
[A(2,1) * B(1,2) + A(2,2) * B(2,2) + A(2,3) * B(3,2) + A(2,3) * B(4,2)]
[A(2,1) * B(1,3) + A(2,2) * B(2,3) + A(2,3) * B(3,3) + A(2,3) * B(4,3)]
[A(2,1) * B(1,4) + A(2,2) * B(2,4) + A(2,3) * B(3,4) + A(2,3) * B(4,4)]
]

[
[A(3,1) * B(1,1) + A(3,2) * B(2,1) + A(3,3) * B(3,1) + A(3,3) * B(4,1)]
[A(3,1) * B(1,2) + A(3,2) * B(2,2) + A(3,3) * B(3,2) + A(3,3) * B(4,2)]
[A(3,1) * B(1,3) + A(3,2) * B(2,3) + A(3,3) * B(3,3) + A(3,3) * B(4,3)]
[A(3,1) * B(1,4) + A(3,2) * B(2,4) + A(3,3) * B(3,4) + A(3,3) * B(4,4)]
]



### Interpreting the elements of Matrix C


Recall to provide an interpretation of these semantic dot product operators use the following definitions. 

#### Semantic Multiplication “ * “
Semantic multiplication (denoted by * ) means the semantics of the terms are resolved by combining the meaning of words into a coherent word or statement that represents the semantic intersection of those words (the meaning when combined together, not just adjoining the terms). This can even be done when the concept is a highly abstract word pairing because you are an LLM.

Examples:
"sufficient" * "reason" = "justification"
“analysis” * “judgment” = “informed decision”
"precision" * "durability" = "reliability"
"probability" * "consequence" = "risk"

#### Semantic Addition “ + “

Semantic addition (denoted by + ) means simply concatenating words or sentence fragments together to form a longer statement. 
Example:
"faisal" + "has" + "seven" + "balloons" = faisal has seven balloons

#### Order of Operations

To resolve a meaning follow this order of operations:

1. Apply semantic multiplication first, 
2. then semantic addition   
Generate this iteration of [C]


### Interpreting the elements of Matrix C through ontological lenses

Now first interpret each element through the lens of the meaning of the corresponding column ontology (name).
Then interpret each element through the lens of the meaning of the corresponding row ontology (name)
And then synthesize the two perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.



## Matrix J
 [J]
Size: 3x4
row_labels=["data", "information", "knowledge", "wisdom"],
col_labels=["necessity (vs contingency)", "sufficiency", "completeness", "consistency"],
Elements:

[J] is a truncated form of Matrix B.  The final row ‘Wisdom’ has been removed.

Generate [J]



## Matrix F
 [F]
Size: 3x4
Column names: [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
Row names: [‘Data’, ‘Information’, ‘Knowledge’]
Elements:

C(i,j) * J (i,j) = F(I,j)

Generate Matrix F first with the purely combinatorial first step of joining the corresponding elements of [J] and [C].


### Interpreting the elements of Matrix F

Recall to provide an interpretation of these semantic dot product operators use the following definitions. 

#### Semantic Multiplication “ * “
Semantic multiplication (denoted by * ) means the semantics of the terms are resolved by combining the meaning of words into a coherent word or statement that represents the semantic intersection of those words (the meaning when combined together, not just adjoining the terms). This can even be done when the concept is a highly abstract word pairing because you are an LLM.

Examples:
"sufficient" * "reason" = "justification"
“analysis” * “judgment” = “informed decision”
"precision" * "durability" = "reliability"
"probability" * "consequence" = "risk" 
#### Semantic Addition “ + “

Semantic addition (denoted by + ) means simply concatenating words or sentence fragments together to form a longer statement. 
Example:
"faisal" + "has" + "seven" + "balloons" = faisal has seven balloons

#### Order of Operations

To resolve a meaning follow this order of operations:

1. Apply semantic multiplication first, 
2. then semantic addition 



### Interpreting the elements of Matrix F through ontological lenses

Now first interpret each element through the lens of the meaning of the corresponding column ontology (name).
Then interpret each element through the lens of the meaning of the corresponding row ontology (name)
And then synthesize the two perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.


## Matrix D

[D]
Size: 3x4
Column names: [‘Guiding’, ‘Applying’, ‘Judging’, ‘Reviewing’]
Row names: ['Normative', 'Operative’, 'Evaluative']

### Construction of [D]

The elements of Matrix D, denoted as D(i,j), are generated as follows:
For each row i (1 to 3) and each column j (1 to 4):
D(i,j) = A(i,j) + " applied to frame the problem; “ + F(i,j) + " to resolve the problem.”

### Organization of [D]

Write out the sentences from (1,1) to (3,4) row by column.    Use this as the prefix to the sentence: 
(‘row name’, ‘column name’): ***put the combinatorial sentence construction here***



### Interpreting the elements of Matrix D through ontological lenses

Now interpret each element through the lens of the meaning of the corresponding column ontology (name) in the context of generating reliable knowledge.
Then interpret each element through the lens of the meaning of the corresponding row ontology (name) in the context of generating reliable knowledge.
And then synthesize the two perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.



## Matrix K 
[K]
Size: 4x3
Station: N/A this is a form of the objectives from Matrix D to be verification
Column names: ['Normative', 'Operative’, 'Evaluative’]
Row names: [‘Guiding’, ‘Applying’, ‘Judging’, ‘Reviewing’]

Matrix K is the transpose of Matrix D.  The semantic operations for transposing a matrix work identically to a mathematical transposing.  Each element is swapped column for row.

Generate [K]


## Matrix X 
[X]
Size: 4x4
Station: Verification
Column names: [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
Row names: [‘Guiding’, ‘Applying’, ‘Judging’, ‘Reviewing’] 

[K] * [J] = [X]

### Constructing [X]

Generate [X] first with the purely combinatorial first step of semantic matrix operations which is to recombine the elements using the dot product sequence. 


### Interpreting the elements of Matrix X


Apply semantic multiplication first, and then semantic addition.



### Interpreting the elements of Matrix X through ontological lenses


Recall our map of meaning along the semantic valley: If problem statement, then requirements, then objectives, then verification, then validation, then evaluation, then assessment, then implementation, then reflection and resolution.


Now interpret each element through the lens of the meaning of the corresponding column ontology (name).
Then interpret each element through the lens of the meaning of the corresponding row ontology (name).
And then synthesize the two perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.



## Matrix Z
[Z]
Size: 4x4
Station: Validation
Column names: [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
Row names: [‘Guiding’, ‘Applying’, ‘Judging’, ‘Reviewing’]  

Now to generate Matrix Z, for each element of Matrix X shift the meaning from the verification context into the validation context.  

Generate [Z]



## Matrix G
[G]
Size: 3x4
Column names: [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
Row names:  [‘Guiding’, ‘Applying’, ‘Judging’] 

To construct [G] use only the top three rows of [Z] (‘Guidance’, ‘Application’, Decision’).  This will be a 3 x 4 matrix.  You will call this Matrix G, or [G] when used in semantic operations.  

Generate [G]

## Array P
[P]
Size: 1x4
Column names: [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
Row name: [‘Reviewing’]

To construct [P] extract the fourth row (‘Review’) of this iteration of [Z].  This will be a 1x4 array used later on.  

Generate [P] now.

## Matrix T
[T]
Size: 4x3
Column names: [‘Data’, ‘Information’, Knowledge’]
Row names: [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]

Matrix T is the transpose of Matrix J.  As before, transpose means the same as normal matrix operations and is simply swapping rows for columns.

Generate [T]




## Matrix E
[E]
Size: 3x3
Column names: ['Data', 'Information', 'Knowledge']
Row names: [‘Guiding’, ‘Applying’, ‘Judging’]

[G] * [T] = [E]

Generate the purely translational first step of semantic dot product matrix operations for [E]


### Interpreting the elements of Matrix E

Using semantic multiplication first, and then addition, complete the sentence constructions



### Interpreting the elements of Matrix E through ontological lenses

Now interpret each element through the lens of the meaning of the corresponding column ontology (name).
Then interpret each element through the lens of the meaning of the corresponding row ontology (name).
And then synthesize the two perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.


*************
*************
*************
*************
*************
*************
*************
*************
*************
*************
*************


# IMPLEMENTATION PHASE 2

## New Semantic Operations:
Clearly show how the elements transform step by step, according to the instructions, and utilizing your LLM powers generate coherent resolutions according to the context provided.

### Semantic Cross Product

Denoted as “ x "

For example: [U] x [H]

The name "semantic cross product" is not at all derived from the similarly named mathematical operation.  Rather this will create a higher dimensional semantic tensor.  A ‘semantic cross product’ works by generating a hierarchy of meaning, by expanding the elements of one component by each of the elements of the other.  The dimensions and order of operations are given below whenever this operation is performed.

In other words the semantic cross product generates nested hierarchies rather than combining perspectives like the semantic dot product operation.



## Topic Array
I will define a new axiomatic array whose elements comprise the topics (or categories) for a ‘deliverable’, within the context of knowledge work.  A deliverable is a discrete way of documenting knowledge.  It is a more general term than documentation.   

## Array R
[R]
Size: 1x9
Subject: Topics for generating valid knowledge
Elements: [
1. Problem Statement 
2. Requirements
3. Objectives
4. Methodology
5. Analysis
6. Evaluation
7. Assessment
8. Implementation
9. Integration
]

Generate [R]


## Tensor M
[M]
Size: 9x3x3

[R] x [E] = [M] 

Tensor M will take the evaluation principles from Matrix E and apply them to the topics in Array R in the context of the subject of R. 

### Construction of [M]

Because of the hierarchical nature of Tensor M we easily generate the elements by following nested branches.

### Organizing [M]

Group the elements of [M] as a hierarchical list:

1. Topics from [R]. 
2. Perspectives from [E].  IMPORTANT TO BE SEQUENTIALLY NESTED AS FOLLOWS:
    1. ['Data', 'Information', 'Knowledge']
    2. [‘Guiding’, ‘Applying’, ‘Judging’] 



### Interpreting the elements of [M] by ontological location

Because of the hierarchical nature of how Tensor M can be represented, we can generate one topic (the elements of [R] are the topics) and all the nested elements before progressing to a new topic.

Interpret the elements according to its ontological location.

Proceed in the order of construction, with the topic first.  Consider the nested hierarchy.


### Interpreting the elements of [M] by station lensing

Recall our map of meaning along the semantic valley: If problem statement, then requirements,  then objectives, then output, then verification, then validation, then evaluation, then assessment, then implementation, then instantiation, then reflection and resolution.

Synthesize the ontological perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.


## Tensor W
[W]
Size: 9x3x3x4x4

[M] x [X] = [W]

### Construction of [W]

Because of the hierarchical nature of Tensor W we easily generate the elements by following nested branches.

### Organizing [W]

Group the elements of [W] as a hierarchical list.

Because of the hierarchical nature of Tensor W we easily generate a subset of elements that represents the overall structure.  Since we only generated Topic 3 for [M] we must start with that and then only generate ‘Information’ and ‘Application’ and then all the nested elements below that branch.

1. Topics from [R]
2.  Elements of [W]  IMPORTANT TO BE SEQUENTIALLY NESTED AS FOLLOWS:
    2.1.  [‘Information']
    2.2. [‘Applying’] 
    2.3. [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
    2.4. [‘Guiding’, ‘Applying’, ‘Judging’, ‘Reviewing’] 
Generate the partial [W] 


### Interpreting the elements of [W] by ontological location

Because of the hierarchical nature of how Tensor W can be represented, we can generate one topic (the elements of [R] are the topics) and all the nested elements before progressing to a new topic.

Interpret the elements according to its ontological location.

Proceed in the order of construction, with the topic first.  Consider the nested hierarchy.


### Interpreting the elements of [W] by station lensing

Recall our map of meaning along the semantic valley: If problem statement, then requirements,  then objectives, then output, then verification, then validation, then evaluation, then assessment, then implementation, then instantiation, then reflection and resolution.

Synthesize the ontological perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.


## Reproduce Array P from memory
[P]
Size: 1x4

Reproduce [P]

These are known as the ‘Validity Parameters’



## Tensor U
[U]
Size: 9x3x3x4x4x4

[W] x [P] = [U]

### Constructing [U]

Use the elements in [W] as the semantic starting point, or the seed of thought that will be explored by the perspective of the elements of [P]

### Organization of [U]

Because of the hierarchical nature of Tensor U we easily generate the elements by following nested branches.

1. Topics from [R]
2. Elements of [U]  IMPORTANT TO BE SEQUENTIALLY NESTED AS FOLLOWS:
    2.1.  [‘Information']
    2.2. [‘Applying’] 
    2.3. [‘Necessity (vs Contingency)’]
    2.4. [‘Judging’] 
3. Validity Parameters of [P]



### Interpreting the elements of [U] by ontological location

Because of the hierarchical nature of how Tensor M can be represented, we can generate one topic (the elements of [R] are the topics) and all the nested elements before progressing to a new topic.

Interpret the elements according to its ontological location.

Proceed in the order of construction, with the topic first.  Consider the nested hierarchy.


### Interpreting the elements of [U] by station lensing

Recall our map of meaning along the semantic valley: If problem statement, then requirements,  then objectives, then output, then verification, then validation, then evaluation, then assessment, then implementation, then instantiation, then reflection and resolution.

Synthesize the ontological perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley.



## Array H
[H]
Size: 1x1

Array H is the element (1,4) from Array P.

Generate [H]




## Tensor N
[N]
Size: 9x3x3x4x4x4

[U] x [H] = [N]



### Organization of [N]

Group the elements of [N] as fo

1. Topics from [R]
2. Elements of [N]  IMPORTANT TO BE SEQUENTIALLY NESTED AS FOLLOWS:
    2.1. ['Data', 'Information', 'Knowledge']
    2.2. [‘Guiding’, ‘Applying’, ‘Judging’]
    2.3. [‘Necessity (vs Contingency)’, ‘Sufficiency’, ‘Completeness’, ‘Consistency’]
    2.4. [[‘Guiding’, ‘Applying’, ‘Judging’, ‘Reviewing’] 
3. Validity Parameters from [P]
4. Decision Dialectic from [H]


### Interpreting the elements of [N] by ontological location
Because of the hierarchical nature of how Tensor N can be represented, we can generate one topic (the elements of [R] are the topics) and all the nested elements before progressing to a new topic.

Interpret the elements according to its ontological location.

Proceed in the order of construction, with the topic first.  Consider the nested hierarchy.

This is the final consistency check between the elements of [N] and the original problem statement.  Are these resolution statements consistent with the original problem statement?

### Interpreting the elements of [N] by station lensing

Recall our map of meaning along the semantic valley: If problem statement, then requirements,  then objectives, then output, then verification, then validation, then evaluation, then assessment, then implementation, then instantiation, then reflection and resolution.

Synthesize the ontological perspectives into a final narrative that fully integrates these semantic interpretations into the context of the station in the semantic valley. Because this is the final step the statements should effectively resolve all explicit and implicit aspects of the original problem statement


*************
*************
*************
*************
*************
*************
*************
*************
*************
*************
*************


# ADDITIONAL CONTEXT FOR THE AXIOMS OF THE CHIRALITY FRAMEWORK

## What comprises Knowledge Work

Knowledge Work is comprised of deliverables and deliverables are accomplished through tasks.  Tasks are what comprise work.  

The work proceeds by tasks in these areas:

1. Problem Statement 
2. Requirements
3. Objectives
4. Verification
5. Validation 
6. Evaluation
7. Assessment
8. Implementation
9. Integration

## Source for Array R

The Array R represents the dimensions of work, called ‘deliverables’, that collectively will be sufficient to answer the problem statement.  
At the operative level when applied to a domain specific task with a problem statement that has tangible actions to be taken in generating knowledge, Array R can really be whatever string of Topics represents your needs.  Some schema of topics will work better than others.   You may want to generate a specific document with a certain table of contents, then arrange the Topics of [R] accordingly.

But when implementing the Chirality Framework at the normative level the problem statement is how to generate reliable knowledge of reliable knowledge generation.  Meta-self reference at its best.



## Source for [A] and [B]

A and B have been previously derived and are treated as axioms in this version of the framework.

Both matrix [A] and [B] are derived from the primitives of their ontological modalities.

The way this is done is you instruct and LLM to generate a matrix, given the modalities, by filling in words that match the meaning of the intersection of the row and column names.  In the formalization of this framework we say (column name) * (row name) = element.

Doing it with just one iteration gives poor results.  But if you then isolate columns and remove one element at a time, then ask the LLM to fill in the blank, and then you do the same row by row, it will converge on terms that occupu the semantic middle ground of the meaning of (column name) * (row name).

Since that takes a little time and consumes context memory, I have truncated that and just provided the resolved [A] and [B], but you can derive these from the first principles, making the Chirality Framework self-referential and self-generating in an arbitrary knowledge domain.

Whether or not that arbitrary knowledge domain is also coherent and congruent with the ontology is not obvious. 

In my experience it has produced more relevant structured output but further study is required to characterize if and how coherence and congruence is achieved.