# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

The Chirality Framework is a "semantic calculator" that implements a fixed, canonical algorithm for structured problem-solving. It's NOT a general-purpose framework but a specific implementation of a three-stage interpretation pipeline that transforms mechanical combinations into meaningful semantic insights.

**Core Philosophy**: Fixed ontological structure + constrained stochastic processing = reproducible semantic computation.

## Development Commands

### Testing
```bash
# Run all tests (uses mock resolvers - fast, no API calls)
pytest

# Run tests with verbose output
pytest -v

# Run specific test file
pytest tests/core/test_operations.py -v

# Run with coverage
pytest --cov=chirality

# Type checking (strict mode enabled)
mypy chirality/

# Code formatting
black chirality/ tests/
```

### CLI Development & Testing
```bash
# Basic cell computation (echo resolver - deterministic, no API calls)
python3 -m chirality.cli compute-cell C --i 0 --j 0 --verbose

# Live OpenAI testing (requires OPENAI_API_KEY env var)
python3 -m chirality.cli compute-cell C --i 0 --j 0 --resolver openai --verbose

# Different matrix types
python3 -m chirality.cli compute-cell F --i 1 --j 2 --verbose
python3 -m chirality.cli compute-cell D --i 2 --j 1 --verbose

# Full observability (tracing + Neo4j export)
python3 -m chirality.cli compute-cell C --i 0 --j 0 --trace --neo4j-export --verbose

# Framework info
python3 -m chirality.cli info

# App integration mode (manifest + cells-jsonl-v1 snapshots + final stdout JSON)
python3 -m chirality.cli compute-pipeline \
  --resolver echo \
  --out runs/dev-run-1 \
  --problem-file problem.json \
  --max-seconds 900
```

### Installation & Setup
```bash
# Development setup
pip install -e ".[dev]"

# Optional dependencies
pip install -e ".[openai]"    # For CellResolver
pip install -e ".[neo4j]"     # For working memory export
pip install -e ".[all]"       # All optional dependencies
```

## Architecture Overview

### Core Algorithm: Three-Stage Interpretation Pipeline
All matrix operations (C, F, D, X, E) follow the same universal pipeline:

1. **Stage 1 (Combinatorial)**: Mechanical generation of k-products or direct pairs
2. **Stage 2 (Semantic Resolution)**: LLM resolves concepts via operation-specific strategies
3. **Stage 3 (Combined Lensing)**: Single unified semantic operation combining row × column × station perspectives

**Special Case - Matrix Z**: Uses lean 2-stage pipeline with station shift instead of lensing

**CRITICAL SEMANTIC RULES**:
1. The file `chirality/normative_spec.txt` is the canonical semantic specification and must NEVER be modified. Only notify the user of suggested changes for them to implement - the semantics cannot be decided by AI.
2. **All prompts are part of the normative spec** and as such should NOT be written by any LLM agent ever. Only the user should ever write or edit the semantic content of any prompt.
3. AI assistants can only edit prompt **structuring** (not content) and only at the user's explicit direction.
4. Station briefs provide station-specific context that grounds the LLM at each semantic valley station. These must be written by the user, not generated by AI.
5. The semantic meaning and interpretation of the framework elements are sovereign decisions that belong exclusively to the human operator.

### CRITICAL OpenAI API Requirements

**MANDATORY**: The Chirality Framework uses OpenAI's **Responses API** exclusively.
- **NEVER USE**: `client.chat.completions.create(messages=[...])` - Chat Completions API is FORBIDDEN
- **ALWAYS USE**: `client.responses.create(prompt=..., ...)` - Responses API is REQUIRED
- This has been incorrectly reverted multiple times. This is non-negotiable.
- The framework requires direct prompt control without message role abstractions.
- ANY use of Chat Completions API must be immediately fixed.

### Key Files & Their Roles

#### Core Implementation (`chirality/core/`)
- **`operations.py`**: The "secret sauce" - implements the 3-stage pipeline for all matrix types
  - `compute_cell_C()`, `compute_cell_X()`, `compute_cell_E()`: Matrix multiplication with k-products
  - `compute_cell_F()`: Element-wise multiplication  
  - `compute_cell_D()`: Synthesis using canonical formula
  - `compute_cell_Z()`: Lean 2-stage with station shift
- **`cell_resolver.py`**: New unified LLM interface with combined operations
  - `run_stage2_multiply()`: Semantic multiplication for k-products
  - `run_stage2_elementwise()`: Element-wise semantic operations
  - `run_stage2_addition()`: Mechanical addition for Matrix D
  - `run_combined_lens()`: Single unified lensing operation
  - `run_shift()`: Station context shift for Matrix Z
- **`matrices.py`**: Fixed canonical matrices (A, B, J) - these are constants, not configurable
- **`types.py`**: Core data structures (`Cell`, `Matrix`, `RichResult`, `SemanticContext`)

#### New Prompt System Architecture (`chirality/lib/`)
- **`strategies.py`**: Maps components to stations and operations
  - Component-driven station selection (C→Requirements, D→Objectives, etc.)
  - Operation strategies for each semantic operation type
- **`prompt_registry.py`**: Manages maintainer-authored prompt assets
  - Loads markdown files from `chirality/prompt_assets/`
  - Tracks versioning and checksums for provenance
- **`prompt_builder.py`**: Constructs messages with placeholder substitution
  - `build_combined_lens_messages()`: Creates unified lensing prompts
  - `build_stage2_multiply_messages()`: Creates multiplication prompts
- **`llm_client.py`**: Unified OpenAI client using Responses API
  - Enforces Responses API usage (never Chat Completions)
  - Handles all LLM interactions with consistent interface

#### Supporting Systems
- **`exporters/working_memory_exporter.py`**: Neo4j export with universal provenance schema
- **`tracer.py`**: JSONL tracing for complete observability
- **`validate.py`**: Validation for framework structural integrity (updated for 3-stage)

### Matrix Relationships
```
C = A * B        (3×4 = 3×4 dot 4×4, via k-products + combined lensing)
F = J ⊙ C        (3×4 = 3×4 element-wise + combined lensing)  
D = A + F        (3×4, canonical synthesis formula + combined lensing)
K = transpose(D) (4×3, direct transpose)
X = K * J        (4×4 = 4×3 dot 3×4, via k-products + combined lensing)
Z = shift(X)     (4×4, station context shift from Verification to Validation)
G = Z[0:3, :]    (3×4, first 3 rows of Z)
T = B[0:3, :].T  (4×3, transpose of first 3 rows of B)
P = Z[3, :]      (1×4, fourth row of Z)
E = G * T        (3×3 = 3×4 dot 4×3, via k-products + combined lensing)
```

### Universal Provenance Schema (Updated for Combined Lensing)
All cells now use a simplified 3-stage provenance structure:
- `stage_1_construct`: Construction content (`texts` for k-products, `text` for direct values)
- `stage_2_semantic`: Resolved concepts (`text` - single unified result from semantic operations)
- `stage_3_combined_lensed`: Combined lensing result (`text` - unified row × column × station interpretation)

**Special Cases**:
- **Matrix Z**: Has empty `stage_3_combined_lensed` (uses station shift in stage 2 instead)
- **Matrix D**: Stage 2 uses mechanical addition (no LLM call)

### Testing Strategy
- **`tests/mocks.py`**: `MockCellResolver` with new API methods (`run_stage2_multiply`, `run_combined_lens`, etc.)
- **`tests/core/test_operations.py`**: Tests simplified 3-stage pipeline with combined lensing
- **`tests/core/test_station5_validation.py`**: Tests Z matrix station shift operation
- **`tests/core/test_station6_evaluation.py`**: Tests E matrix evaluation pipeline
- **`tests/integration/test_prompt_system_integration.py`**: Tests new prompt system components
- Use echo resolver for pipeline mechanics testing, OpenAI resolver for semantic validation

### Key Design Principles (Updated)
1. **RichResult Objects**: All resolver methods return structured objects with `text`, `terms_used`, `warnings`, `metadata`
2. **Combined Lensing**: Single unified semantic operation (row × column × station) replacing 3-step lensing
3. **Component-Driven Architecture**: Component ID (C, D, F, X, Z, E) drives station selection and operations
4. **Maintainer-Authored Prompts**: All semantic prompts in `chirality/prompt_assets/` as markdown files
5. **Canonical Problem**: Fixed to "generating reliable knowledge" for D matrix
6. **Lean Z Pipeline**: Matrix Z uses 2-stage with station shift instead of lensing
7. **Single API Call Per Stage**: Each semantic stage makes exactly one LLM call for efficiency

### Critical Development Notes
- Always run tests after changes: `pytest`
- Use echo resolver for development/testing (fast, deterministic)
- OpenAI resolver requires `OPENAI_API_KEY` environment variable
- **Prompt assets are in `chirality/prompt_assets/`** - only maintainer should edit these
- **New resolver API**: Use `run_stage2_multiply()`, `run_combined_lens()`, etc. (not old methods)
- **Provenance is now 3-stage**: `stage_1_construct`, `stage_2_semantic`, `stage_3_combined_lensed`
- Neo4j export preserves both type distinctions (labels) and instance distinctions (unique IDs)
- Never modify canonical matrices (A, B, J) unless updating the fundamental specification

### Common Debugging Workflow
1. Test with echo resolver first to verify pipeline mechanics
2. Test with OpenAI resolver to verify semantic resolution
3. Use `--verbose` flag to see all stage outputs
4. Use `--trace` flag for detailed JSONL logs
5. Use `--neo4j-export` for graph analysis of semantic journeys

## Recent Architecture Changes (Combined Lensing Refactor)

The framework underwent a major refactor to implement combined lensing architecture:

### What Changed
- **Old**: Three-stage lensing (column lens → row lens → synthesis) with 3 separate LLM calls
- **New**: Single combined lensing operation with 1 LLM call per cell
- **Old**: 5-stage provenance (`stage_3_column_lensed`, `stage_4_row_lensed`, `stage_5_final_synthesis`)
- **New**: 3-stage provenance with `stage_3_combined_lensed` containing unified result
- **Old**: Methods like `resolve_semantic_pair()`, `apply_column_lens()`, etc.
- **New**: Unified methods like `run_stage2_multiply()`, `run_combined_lens()`

### Why It Matters
- **Efficiency**: Reduces LLM calls from 4+ per cell to 2 (one for Stage 2, one for Stage 3)
- **Coherence**: Combined lensing produces more integrated interpretations
- **Simplicity**: Cleaner API with fewer methods and simpler provenance structure

### Migration Notes
- All tests have been updated to use new API
- MockCellResolver and EchoResolver updated with new methods
- Validation function updated to expect 3-stage provenance
- CI/CD pipelines configured to enforce new architecture
